<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2018%2F01%2F20%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[基础框架-Maven模块化]]></title>
    <url>%2F2018%2F01%2F20%2Fframe-modular%2F</url>
    <content type="text"><![CDATA[这个是maven分模块的工程。 IDE设置 git clone 代码 mvn install 在工作目录中使用mvn eclipse:eclipse[Eclipse]或mvn idea:idea[Intellij IDEA]初始化工程 在IDE中导入已有工程 打包与运行 打包jar命令：mvn package -P dev[开发环境配置]，mvn package -P online[生产环境配置]其中，开发环境dev的配置位于src/main/resources/dev中，生产环境online的配置位于src/main/resources/online中默认为dev开发环境的配置 运行其中datadriver-web中是war包，该包中包含有其他模块的jar在lib目录中]]></content>
      <categories>
        <category>Datadriver</category>
      </categories>
      <tags>
        <tag>black magic</tag>
        <tag>fucking trick</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[知乎爬虫]]></title>
    <url>%2F2018%2F01%2F20%2Fzhihu-crawler%2F</url>
    <content type="text"><![CDATA[zhihu-crawler是一个基于Java的爬虫实战项目，主要功能是抓取知乎用户的基本资料，如果觉得不错，请给个star。 爬取结果 下图为爬取117w知乎用户数据的简单统计 详细统计见 https://www.vwycm.cn/zhihu/charts 工程导入(maven) git clone https://github.com/wycm/zhihu-crawler 克隆项目到本地 eclipse导入步骤(eclipse_kepler版本，自带maven)，File-&gt;Import-&gt;Maven-&gt;Existing Maven Projects-&gt;选择刚刚clone的zhihu-crawler目录-&gt;导入成功 idea导入步骤,File-&gt;Open-&gt;选择刚刚clone的zhihu-crawler目录-&gt;导入成功 工程导入(不使用maven) eclipse或myeclipse都可以 git clone https://github.com/wycm/zhihu-crawler 克隆项目到本地 创建一个名字为zhihu-new-crawler（这个随便命名）的普通工程 拷贝刚刚clone的zhihu-crawler/src/main/java/com目录到zhihu-new-crawler/src目录下 右键工程-&gt;Build Path-&gt;Add External Archives…-&gt;导入zhihu-crawler/lib下的所有jar包 将zhihu-crawler/src/main/resources目录下的config.properties和log4j.properties拷贝至src目录下 Quick StartRun with Main.java 使用到的API 地址(url)：12345678910* 请求类型：GET* **请求参数**| 参数名 |类型 | 必填 | 值 | 说明|| :------------ | :------------ | :------------ | :----- | :------------ || include | String | 是| ```data[*]answer_count,articles_count``` |需要返回的字段（这个值可以改根据需要增加一些字段，见如下示例url） || offset | int | 是| 0 | 偏移量（通过调整这个值可以获取到一个用户的```所有关注用户```资料） || limit | int | 是| 20 | 返回用户数（最大20，超过20无效） |* url示例：```https://www.zhihu.com/api/v4/members/wo-yan-chen-mo/followees?include=data[*].educations,employments,answer_count,business,locations,articles_count,follower_count,gender,following_count,question_count,voteup_count,thanked_count,is_followed,is_following,badge[?(type=best_answerer)].topics&amp;offset=0&amp;limit=20 响应：json数据，会有关注用户资料 注意：这个请求采用了oauth验证，需要在http header加上authorization:oauth c3cef7c66a1843f8b3a9e6a1e3160e20,这个值是存放在js文件中，详细获取方式见代码。 使用到的三方库 HttpClient4.5-网络请求 Jsoup-html标签解析 JsonPath-json数据解析 Features 大量使用http代理，突破同一个客户端访问量限制。 支持持久化(mysql),相关配置见config.properties。 多线程、爬取速度快。 持久化配置 如果要持久化数据到mysql，先手动创建数据库，然后修改配置文件config.properties的如下属性。 db.enable = true db.host =ip地址 db.username = 用户名 db.password = 密码 db.name = 数据库名 流程图 更新2016.12.26 移除未使用的包，修复ConcurrentModificationException和NoSuchElementException异常问题。 增加游客（免登录）模式抓取。 增加代理抓取模块。 2017.01.10 不再采用登录抓取，并移除登录抓取相关模块，模拟登录的主要逻辑代码见ModelLogin.java。 优化项目结构，加快爬取速度。采用ListPageThreadPool和DetailPageThreadPool两个线程池。ListPageThreadPool负责下载”关注用户“列表页，解析出关注用户，将关注用户的url去重，然后放到DetailPageThreadPool线程池。DetailPageThreadPool负责下载用户详情页面，解析出用户基本信息并入库，获取该用户的”关注用户”的列表页url并放到ListPageThreadPool。 2017.01.17 增加代理序列化。 调整项目结构，大幅度提高爬取速度。不再使用ListPageThreadPool和DetailPageThreadPool的方式。直接下载关注列表页，可以直接获取到用户详细资料。 2017.03.30 知乎api变更，关注列表页不能获取到关注人数，导致线程池任务不能持续下去。抓取模式切换成原来ListPageThreadPool和DetailPageThreadPool的方式。 2017.05.26 修复代理返回错误数据，导致java.lang.reflect.UndeclaredThrowableException异常。 2017.11.05 知乎authorization文件更新，修改authorization获取方式。TODO 优化爬取速度 最后 想要爬取其它数据，如问题、答案等，完全可以在此基础上自己定制。 有问题的请提issue。 欢迎贡献代码。 爬虫交流群：633925314，欢迎交流。]]></content>
      <categories>
        <category>github</category>
      </categories>
      <tags>
        <tag>black magic</tag>
        <tag>fucking trick</tag>
      </tags>
  </entry>
</search>
